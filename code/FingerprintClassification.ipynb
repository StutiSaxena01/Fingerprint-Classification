{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "from scipy import signal\n",
    "from skimage.filters import gabor_kernel\n",
    "from scipy import ndimage as nd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.ndimage import grey_dilation as gd, rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Fingerprint Normalization\n",
    "def segmentImg(img,blockSize,threshold,display=False):\n",
    "    global SMALL_VAL\n",
    "    rows,cols=img.shape\n",
    "    threshold=np.std(img)*threshold\n",
    "    varImg=np.empty(np.shape(img))\n",
    "    mask=np.ones(np.shape(img))\n",
    "    for i in range(0,rows,blockSize):\n",
    "        for j in range(0,cols,blockSize):\n",
    "            endi=min(i+blockSize,rows)\n",
    "            endj=min(j+blockSize,cols)\n",
    "            varImg[i:endi, j:endj]=np.std(img[i:endi, j:endj])\n",
    "    \n",
    "    mask[np.where(varImg<threshold)]=0\n",
    "\n",
    "    # smooth mask with a open/close morphological filter\n",
    "    kernel = cv.getStructuringElement(cv.MORPH_ELLIPSE,(blockSize*2, blockSize*2))\n",
    "    mask = cv.morphologyEx(mask, cv.MORPH_OPEN, kernel)\n",
    "    mask = cv.morphologyEx(mask, cv.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    segmentedImg=img.copy()\n",
    "    segmentedImg=np.multiply(mask,segmentedImg)    \n",
    "\n",
    "    std=np.std(segmentedImg)\n",
    "    if(std==0):\n",
    "        std=SMALL_VAL\n",
    "    img=(segmentedImg-np.mean(segmentedImg))/std\n",
    "    meanVal=np.mean(img[np.where(mask==0)])\n",
    "    stdDev=np.std(img[np.where(mask==0)])\n",
    "    stdDev=np.std(img[np.where(mask==0)])\n",
    "    if(stdDev==0):\n",
    "        stdDev=SMALL_VAL\n",
    "    normalizedImg=(img-meanVal)/stdDev\n",
    "    \n",
    "    \n",
    "    if display:\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.imshow(segmentedImg, cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.imshow(normalizedImg, cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "    return segmentedImg, normalizedImg, mask\n",
    "\n",
    "\n",
    "# img=cv.imread('../NISTSpecialDatabase4GrayScaleImagesofFIGS/sd04/png_txt/figs_0/s0007_09.png',0)         #read as grayscale\n",
    "# plt.figure(figsize=(5,5))\n",
    "# plt.imshow(img, cmap='gray')\n",
    "# plt.show()\n",
    "# segmentedImg, normalizedImg, mask=segmentImg(img,10,0.2,display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getOrientationField(img, blockSize, display=False):\n",
    "    sobelOperator = [[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]\n",
    "    xSobel = np.array(sobelOperator).astype(np.int)\n",
    "    ySobel = np.transpose(xSobel).astype(np.int)\n",
    "\n",
    "    grad_x = cv.filter2D(img,-1, xSobel)\n",
    "    grad_y = cv.filter2D(img,-1, ySobel)\n",
    "\n",
    "    if(display):\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.imshow(grad_x, cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.imshow(grad_y, cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "    # find local field orientation\n",
    "    rows,cols=np.shape(grad_x)\n",
    "    rows1=rows//blockSize\n",
    "    cols1=cols//blockSize\n",
    "    vx=np.empty((rows1,cols1))\n",
    "    vy=np.empty((rows1,cols1))\n",
    "\n",
    "    for x in range(rows1):\n",
    "        startx=x*blockSize\n",
    "        for y in range(cols1):\n",
    "            starty=y*blockSize\n",
    "            gradX=grad_x[startx:startx+blockSize, starty:starty+blockSize]\n",
    "            gradY=grad_y[startx:startx+blockSize, starty:starty+blockSize]\n",
    "            vx[x,y]=(np.sum(2*np.multiply(gradX, gradY)))\n",
    "            vy[x,y]=(np.sum(gradX**2-gradY**2))\n",
    "\n",
    "    if(display):\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.imshow(vx, cmap='gray')\n",
    "        # plt.imshow(vy, cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.imshow(vy, cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "    #theta is orientation field direction in a block\n",
    "    theta=np.empty((rows1,cols1))\n",
    "    for i in range(rows1):\n",
    "        for j in range(cols1):\n",
    "            if(vy[i,j]==0):\n",
    "                theta[i,j]=0\n",
    "            else:\n",
    "                theta[i,j]=0.5*math.atan2(vx[i,j],vy[i,j])\n",
    "\n",
    "    if(display):\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.imshow(theta, cmap='gray')\n",
    "        plt.show()\n",
    "        \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def smoothOrientationField(theta, display=False):\n",
    "    #continuous vector field\n",
    "    phiX=np.empty(np.shape(theta))\n",
    "    phiY=np.empty(np.shape(theta))\n",
    "\n",
    "    phiX=np.cos(2*theta)\n",
    "    phiY=np.sin(2*theta)\n",
    "\n",
    "    h=np.ones((3,3))\n",
    "    h=h/9\n",
    "    fieldX=signal.convolve2d(phiX, np.rot90(h), mode='same')\n",
    "    fieldY=signal.convolve2d(phiY, np.rot90(h), mode='same')\n",
    "\n",
    "    if(display):\n",
    "        plt.figure(figsize=(3,3))\n",
    "        plt.imshow(fieldX, cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(3,3))\n",
    "        plt.imshow(fieldY, cmap='gray')\n",
    "        plt.show()\n",
    "        \n",
    "    smoothed=np.zeros_like(theta)\n",
    "    smoothed=0.5*np.arctan2(fieldY,fieldX)\n",
    "\n",
    "    if(display):\n",
    "        plt.figure(figsize=(3,3))\n",
    "        plt.imshow(smoothed, cmap='gray')\n",
    "        plt.show()\n",
    "        \n",
    "    return smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findBlobs(rowList, colList, img):\n",
    "    #return startpos and area of the blob which is geographically upward if 2 or less blobs are found\n",
    "    #else return number of blobs. If no blob do nothing on return else smooth the img\n",
    "    #if atleast one blob of area 4 is found, discard smaller blobs\n",
    "    blobs=[]\n",
    "    blobArea=[]\n",
    "    points=list(zip(rowList, colList))\n",
    "    points = sorted(points) \n",
    "    while(len(points)!=0):\n",
    "        point=points[0]\n",
    "        area=0\n",
    "        temprow, tempcol=point\n",
    "        while((temprow, tempcol) in points): \n",
    "            area+=1\n",
    "            points.remove((temprow,tempcol))\n",
    "            while((temprow,tempcol+1) in points):\n",
    "                area+=1\n",
    "                points.remove((temprow,tempcol+1))\n",
    "                tempcol+=1\n",
    "\n",
    "            tempcol=point[1]\n",
    "            temprow+=1\n",
    "            \n",
    "        blobs.append((area, point[0], point[1]))\n",
    "        blobArea.append(area)\n",
    "        \n",
    "    return blobs, np.array(blobArea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def findPoincareVal(smoothed, i, j):\n",
    "    global cells\n",
    "    angles=[math.degrees(smoothed[i-m][j-n])%180 for m,n in cells]\n",
    "    poincare=0\n",
    "    for k in range(0,8):\n",
    "        smallDelta=angles[k+1]-angles[k]\n",
    "        if(smallDelta<90 and smallDelta>-90):\n",
    "            poincare+=smallDelta\n",
    "        elif(smallDelta<=-90):\n",
    "            poincare+=(180+smallDelta)\n",
    "        else:\n",
    "            poincare+=(smallDelta-180)\n",
    "    return poincare\n",
    "\n",
    "def getCorePoint(normalizedImg, theta, display=False):\n",
    "    global smooth, BLOCKSIZE\n",
    "    rows,cols=theta.shape\n",
    "    pointX=0\n",
    "    pointY=0\n",
    "    count=0\n",
    "    A=np.empty((rows-2, cols-2))\n",
    "    for i in range(1,rows-1):\n",
    "        for j in range(1,cols-1):\n",
    "            A[i-1,j-1]=findPoincareVal(theta, i, j)\n",
    "\n",
    "    #normalise poincare values\n",
    "    maxA=np.max(A)\n",
    "    minA=np.min(A)\n",
    "    if(maxA != minA):\n",
    "        A=(A - minA)/(maxA - minA)\n",
    "    #white corresponds to core point\n",
    "    B=A[6:-6, 6:-6]\n",
    "\n",
    "    B=np.round(B, decimals=3)\n",
    "\n",
    "    if(display):\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.imshow(A, cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "    xList=np.where((B<=1)&(B>0.66))[0]+6                   #x is the row; y is the col\n",
    "    yList=np.where((B<=1)&(B>0.66))[1]+6\n",
    "#     print(xList, yList)\n",
    "\n",
    "    blobs, blobArea = findBlobs(xList,yList,B)\n",
    "\n",
    "#     if(len(np.where(blobArea>=3)[0]<=2) or len(blobs)==1):\n",
    "    if(len(blobs)==0):\n",
    "        return None, None\n",
    "    elif((len(np.where(blobArea>=3)[0])<=2 and len(blobs)<=5) or len(blobs)==1):\n",
    "        blobs=sorted(blobs, key=lambda x: (-x[0], x[1], x[2]))\n",
    "        inc=np.sqrt(blobs[0][0])/2\n",
    "        pointX,pointY=blobs[0][1]+inc, blobs[0][2]+inc\n",
    "    else:\n",
    "        smooth=True\n",
    "        return None, None\n",
    "\n",
    "#     print(pointY, pointX)\n",
    "\n",
    "    #get point location in original image\n",
    "    coreX=pointX*BLOCKSIZE+BLOCKSIZE/2\n",
    "    coreY=pointY*BLOCKSIZE+BLOCKSIZE/2\n",
    "\n",
    "    #center is 40 pixels below core\n",
    "#     centerX=coreX+40\n",
    "#     centerY=coreY\n",
    "\n",
    "    if(display):\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.scatter(coreY,coreX, color='blue')\n",
    "#         plt.scatter(centerY,centerX, color='red')\n",
    "        plt.imshow(normalizedImg, cmap='gray')\n",
    "        plt.show()\n",
    "        \n",
    "    return A,(coreY, coreX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getROI(normalizedImg, centerX, centerY, display=False, concCircles=6, k=8):\n",
    "    '''\n",
    "    returns sectorArr: array containing the sector each pixel belongs to\n",
    "            roi: 140px around the center point in all directions\n",
    "    '''\n",
    "    rows,cols=np.shape(normalizedImg)\n",
    "\n",
    "    startX=centerX-140\n",
    "    startY=centerY-140\n",
    "    endX=centerX+140\n",
    "    endY=centerY+140\n",
    "    if(endY>cols or endX>rows or startX<0 or startY<0):\n",
    "        return None, None\n",
    "\n",
    "    centerX=centerY=140\n",
    "#     if(centerX-145>=0):\n",
    "#         centerX=145\n",
    "#     if(centerY-145>=0):\n",
    "#         centerY=145\n",
    "\n",
    "    roi=normalizedImg[int(startX):int(endX), int(startY):int(endY)]\n",
    "\n",
    "    sectorTheta=[]\n",
    "    sectorT=[]\n",
    "    b=20     #distance between concentric circles around the center\n",
    "\n",
    "    for sector in range(k):\n",
    "        th_i=(sector%k)*2*180/k\n",
    "        sectorTheta.append(th_i)\n",
    "\n",
    "    sectorTheta.append(360)\n",
    "\n",
    "    for sector in range(concCircles+1):\n",
    "        sectorT.append(b*(sector+1))\n",
    "\n",
    "    center=np.array([centerX,centerY])\n",
    "    rows, cols=np.shape(roi)\n",
    "    sectorArr=np.empty(np.shape(roi))\n",
    "    sectorArr.fill(-1)\n",
    "\n",
    "    for row in range(0,rows):\n",
    "        for col in range(0,cols):\n",
    "            curPoint=np.array([row,col])\n",
    "            r=np.linalg.norm(curPoint-center)\n",
    "            th=np.arctan2(-centerX+row,-centerY+col)\n",
    "            th = math.fmod((th * (180 / math.pi)) + 360, 360)\n",
    "            foundv1=False\n",
    "            foundv2=False\n",
    "\n",
    "            for i in range(0,concCircles):\n",
    "                if(foundv1==False and r>=sectorT[i] and r<sectorT[i+1]):\n",
    "                    foundv1=True\n",
    "                    v1=i\n",
    "                    break\n",
    "            for i in range(0,k):\n",
    "                if(foundv2==False and th>=sectorTheta[i] and th<sectorTheta[i+1]):\n",
    "                    foundv2=True\n",
    "                    v2=i\n",
    "                    break\n",
    "\n",
    "            if(foundv1==True and foundv2==True):\n",
    "                sectorArr[row,col]=v1*k+v2\n",
    "    \n",
    "    if(display):\n",
    "        roi[np.where(sectorArr==-1)]=0\n",
    "        figure=plt.figure(figsize=(5,5))\n",
    "        for i in range(1,8):\n",
    "            circle1 = plt.Circle((centerY, centerX), 20*i ,fill = False, color='cyan', linewidth=2)\n",
    "            plt.gca().add_patch(circle1)\n",
    "\n",
    "        for theta in sectorTheta:\n",
    "            x = 140 * np.cos(theta/180*np.pi)\n",
    "            y = 140 * np.sin(theta/180*np.pi)\n",
    "            plt.plot([140,140+x], [140,140+y], color='cyan')\n",
    "        plt.scatter(centerY, centerX, color='red')\n",
    "        plt.scatter(centerY, centerX-40, color='red')\n",
    "        plt.imshow(roi, cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "    return roi, sectorArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def normalizeImg(img, desiredMean=100, desiredVar=100):\n",
    "    #img is a numpy array (1D in this case)\n",
    "    SMALL_VAL=0.00001\n",
    "    meanVal=np.mean(img)\n",
    "    varVal=np.var(img)\n",
    "    if(varVal==0):\n",
    "        varVal=SMALL_VAL\n",
    "    out=np.empty(np.shape(img))\n",
    "    out[np.where(img>=meanVal)]=desiredMean+np.sqrt(desiredVar*(img[np.where(img>=meanVal)]-meanVal)**2/varVal)\n",
    "    out[np.where(img<meanVal)]=desiredMean-np.sqrt(desiredVar*(img[np.where(img<meanVal)]-meanVal)**2/varVal)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projectOrthogonally(block, orientation, minWavelength=5, maxWavelength=15):\n",
    "\n",
    "    freqBlock=np.empty(np.shape(block))\n",
    "\n",
    "    cosOrient=np.cos(2*orientation)\n",
    "    sinOrient=np.sin(2*orientation)\n",
    "    meanOrient=0.5*math.atan2(sinOrient, cosOrient)\n",
    "    meanOrientDeg=meanOrient*180/np.pi\n",
    "    \n",
    "    projectedBlock=rotate(block, meanOrientDeg+90, axes=(1,0), reshape=False, mode='nearest')\n",
    "    \n",
    "    #sum of pixel values along the ridges (which are vertical now)\n",
    "    projSum=np.sum(projectedBlock, axis=0)\n",
    "    \n",
    "    #grey-dilation: maximum filter over a sliding window - will give local maximum\n",
    "    localMax=gd(projSum, 5, structure=np.ones(5))\n",
    "    \n",
    "    #diff between local maxima and cur value < threshold then maxima\n",
    "    noiseThresh=2\n",
    "    diff=np.abs(projSum-localMax)\n",
    "    maximaInd=np.where((projSum>np.mean(projSum)) & (diff<noiseThresh))[0]\n",
    "    numPeaks=len(maximaInd)\n",
    "    if(numPeaks<=1):\n",
    "        freqBlock.fill(0)\n",
    "    else:\n",
    "        wavelength=(maximaInd[-1]-maximaInd[0])/(numPeaks-1)\n",
    "        if(wavelength<minWavelength or wavelength>maxWavelength):\n",
    "            freqBlock.fill(0)\n",
    "        else:\n",
    "            freqBlock.fill(1.0/wavelength)\n",
    "    return freqBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFreqMap(normalizedImg, smoothed, blockSize, mask):\n",
    "    freqMap=np.empty(np.shape(normalizedImg))\n",
    "    rows, cols=np.shape(normalizedImg)\n",
    "\n",
    "    for row in range(0, rows-blockSize, blockSize):\n",
    "        for col in range(0, cols-blockSize, blockSize):\n",
    "            #blockOrientation is the local ridge orientation\n",
    "            blockOrientation=smoothed[row//blockSize, col//blockSize]\n",
    "            freqMap[row:row+blockSize, col:col+blockSize]=projectOrthogonally(normalizedImg[row:row+blockSize, col:col+blockSize], blockOrientation)\n",
    "    \n",
    "    freqMap=freqMap*mask\n",
    "    freqMap=freqMap.reshape(-1)\n",
    "    return freqMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGaborFilters(medianFreq, blockSize, display=False, kx=0.5, ky=0.5):\n",
    "    #check sigma_x=medianFreq*kx\n",
    "    sigma_x=1.0/medianFreq*kx\n",
    "    sigma_y=1.0/medianFreq*ky\n",
    "    block_size = np.round(3*sigma_x)\n",
    "    numFilters=4\n",
    "\n",
    "    kernelSize=int(block_size*2+1)\n",
    "#     sigma=2.5\n",
    "    sigma=1.0/medianFreq*kx\n",
    "    lamda=1/medianFreq\n",
    "    gamma=1\n",
    "    phi=0.1\n",
    "    gaborFilters=[]\n",
    "    for mul in range(0,numFilters):\n",
    "        theta=np.pi/numFilters*mul+np.pi/2  #horizontal\n",
    "#         print(180/numFilters*mul)\n",
    "        kernel=cv.getGaborKernel((kernelSize, kernelSize), sigma, theta, lamda, gamma, phi)\n",
    "        if(display):\n",
    "            plt.figure(figsize=(3,3))\n",
    "            plt.imshow(kernel, cmap='gray')\n",
    "            plt.show()\n",
    "        gaborFilters.append(kernel)\n",
    "\n",
    "    return gaborFilters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_feats(image, kernels, display=False):\n",
    "    compImgs=[]\n",
    "    feats = np.zeros((len(kernels), 2), dtype=np.double)\n",
    "    for k, kernel in enumerate(kernels):\n",
    "        filtered = nd.convolve(image, kernel)\n",
    "        \n",
    "        if display:\n",
    "            plt.figure(figsize=(5,5))\n",
    "            plt.imshow(filtered, cmap='gray')\n",
    "            plt.show()\n",
    "        compImgs.append(filtered)\n",
    "    return compImgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def computeF(compImg, sectorArr, numSectors):\n",
    "    #return numSectors(48 in this code) values\n",
    "    featVec=np.array([])\n",
    "    for sector in range(numSectors):\n",
    "        indices=np.where(sectorArr==sector)\n",
    "        var=np.sum((compImg[indices]-np.mean(compImg[indices]))**2)\n",
    "        std=np.sqrt(var)\n",
    "        featVec=np.append(featVec, std)\n",
    "    return featVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "path = '../NISTSpecialDatabase4GrayScaleImagesofFIGS/sd04/png_txt/'\n",
    "\n",
    "\n",
    "labels = []\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for each in os.listdir(folder):\n",
    "        for filename in os.listdir(os.path.join(folder,each)):\n",
    "            img = cv.imread(os.path.join(folder,each,filename),0)\n",
    "            if img is not None:\n",
    "                images.append(img)\n",
    "            else:\n",
    "                f = open(os.path.join(folder,each,filename), \"r\", encoding='utf-8')\n",
    "                try:\n",
    "                    Class = ((f.read().splitlines())[1])[-1]\n",
    "                    labels.append(Class)\n",
    "                except:\n",
    "                    pass\n",
    "    return images\n",
    "\n",
    "fingerprints = load_images_from_folder(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx_labels = list(set(labels))\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    labels[i] = idx_labels.index(labels[i])\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handler Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "failedCount = 0\n",
    "failedImgs = []\n",
    "errorMsgs=[]\n",
    "\n",
    "BLOCKSIZE=16\n",
    "cells = [(-1, -1), (-1, 0), (-1, 1), (0, 1), (1, 1), (1, 0), (1, -1), (0, -1), (-1, -1)]\n",
    "SMALL_VAL=0.00001\n",
    "concCircles=6\n",
    "k=8      #number of sectors in each circle\n",
    "desiredMean=100\n",
    "desiredVar=100\n",
    "smooth = False\n",
    "\n",
    "def handler(img, imgPath=None, display=False):\n",
    "    global failedCount\n",
    "    global failedImgs\n",
    "    global smooth\n",
    "    global errorMsgs\n",
    "    global BLOCKSIZE\n",
    "\n",
    "    #imageName=imgPath.split('/')[-1].split('.')[0]\n",
    "\n",
    "    segmentedImg, normalizedImg, mask = segmentImg(img, BLOCKSIZE, 0.1, display)\n",
    "\n",
    "    theta = getOrientationField(normalizedImg, BLOCKSIZE, display)\n",
    "\n",
    "    smooth=False\n",
    "    countSmooth=0\n",
    "    A, core = getCorePoint(normalizedImg, theta, display)\n",
    "    print(core)\n",
    "\n",
    "    while(smooth==True and countSmooth<3):\n",
    "        smooth=False\n",
    "        countSmooth+=1\n",
    "        theta = smoothOrientationField(theta, display)\n",
    "        A, core = getCorePoint(normalizedImg, theta, display)\n",
    "        print(core)\n",
    "        \n",
    "#     if(smooth==True or core is None):\n",
    "    if(core==None):\n",
    "        failedCount+=1\n",
    "        failedImgs.append(imgPath)\n",
    "        errorMsgs.append('-1')           #unable to find core\n",
    "        return None\n",
    "    \n",
    "    coreY, coreX = core[0], core[1]\n",
    "\n",
    "#     saveImage1=normalizedImg.copy()\n",
    "#     saveImage1=cv.circle(saveImage1, (int(coreY),int(coreX)), radius=5, color=(0,255,0), thickness=-1)\n",
    "#     plt.imsave('output/'+imageName+'_1.png', saveImage1)\n",
    "#     plt.imsave('output/'+imageName+'_2.png', cv.resize(A,(512,512)))\n",
    "\n",
    "    #center is 40 pixels below core\n",
    "    centerX=coreX+40\n",
    "    centerY=coreY\n",
    "\n",
    "    roi, sectorArr = getROI(normalizedImg, centerX, centerY, display)\n",
    "    if(sectorArr is None):\n",
    "        failedCount+=1\n",
    "        failedImgs.append(imgPath)\n",
    "        errorMsgs.append('-2')         #corepoint doesn't have 140px space around it in atleast one direction\n",
    "        return None\n",
    "\n",
    "    tempArr=roi.copy()\n",
    "\n",
    "    #normalize each sector\n",
    "    numSectors=concCircles*k\n",
    "    for sector in range(numSectors):\n",
    "        indices=np.where(sectorArr==sector)\n",
    "        tempArr[indices]=normalizeImg(tempArr[indices], desiredMean, desiredVar)\n",
    "\n",
    "    if(display):\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.imshow(tempArr, cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "    #Get freq of each block\n",
    "    freqMap = getFreqMap(normalizedImg, theta, BLOCKSIZE, mask)\n",
    "    posFreqInd = np.where(freqMap>0)[0]\n",
    "    posFreq=freqMap[posFreqInd]\n",
    "    medianFreq=np.median(posFreq)\n",
    "\n",
    "    #apply 4 gabor kernels to the image oriented at 0, 45, 90 and 135 deg\n",
    "    \n",
    "    #get kernels\n",
    "    if(medianFreq != 0):\n",
    "        kernels=getGaborFilters(medianFreq, BLOCKSIZE, display)\n",
    "    else:\n",
    "        failedCount+=1\n",
    "        failedImgs.append(imgPath)\n",
    "        errorMsgs.append('-3')         #medianfreq is 0        \n",
    "        return None\n",
    "    \n",
    "    #apply to the image\n",
    "    compImgs=compute_feats(tempArr,kernels,display)    \n",
    "    \n",
    "    #get feature vector\n",
    "    featureVector=np.array([])\n",
    "    for i in range(len(compImgs)):\n",
    "        featVec=computeF(compImgs[i], sectorArr, numSectors)\n",
    "        featureVector=np.concatenate((featureVector, featVec))    \n",
    "    \n",
    "    return featureVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Discarded 3904\n",
      "(248.0, 184.0)\n",
      "Done... 3905\n",
      "(147.31370849898477, 227.31370849898477)\n",
      "Done... 3906\n",
      "None\n",
      "Discarded 3907\n",
      "(200.0, 184.0)\n",
      "Done... 3908\n",
      "(280.0, 168.0)\n",
      "Done... 3909\n",
      "None\n",
      "Discarded 3910\n",
      "(232.0, 216.0)\n",
      "Done... 3911\n",
      "(328.0, 264.0)\n",
      "Done... 3912\n",
      "(232.0, 136.0)\n",
      "Done... 3913\n",
      "(312.0, 184.0)\n",
      "Done... 3914\n",
      "None\n",
      "Discarded 3915\n",
      "(264.0, 200.0)\n",
      "Done... 3916\n",
      "None\n",
      "Discarded 3917\n",
      "(235.59591794226543, 235.59591794226543)\n",
      "Done... 3918\n",
      "None\n",
      "Discarded 3919\n",
      "None\n",
      "None\n",
      "Discarded 3920\n",
      "None\n",
      "(251.59591794226543, 171.59591794226543)\n",
      "Done... 3921\n",
      "(131.31370849898477, 179.31370849898477)\n",
      "Discarded 3922\n",
      "(245.30817386124556, 245.30817386124556)\n",
      "Done... 3923\n",
      "None\n",
      "Discarded 3924\n",
      "(232.0, 184.0)\n",
      "Done... 3925\n",
      "None\n",
      "Discarded 3926\n",
      "None\n",
      "Discarded 3927\n",
      "(248.0, 216.0)\n",
      "Done... 3928\n",
      "(232.0, 216.0)\n",
      "Done... 3929\n",
      "None\n",
      "(264.0, 168.0)\n",
      "Done... 3930\n",
      "(339.31370849898474, 195.31370849898477)\n",
      "Done... 3931\n",
      "None\n",
      "Discarded 3932\n",
      "(243.9428454762872, 243.9428454762872)\n",
      "Done... 3933\n",
      "(248.0, 216.0)\n",
      "Done... 3934\n",
      "(291.31370849898474, 275.31370849898474)\n",
      "Done... 3935\n",
      "(376.0, 200.0)\n",
      "Discarded 3936\n",
      "None\n",
      "Discarded 3937\n",
      "None\n",
      "Discarded 3938\n",
      "(216.0, 200.0)\n",
      "Done... 3939\n",
      "(216.0, 248.0)\n",
      "Done... 3940\n",
      "(264.0, 280.0)\n",
      "Done... 3941\n",
      "(216.0, 168.0)\n",
      "Done... 3942\n",
      "None\n",
      "Discarded 3943\n",
      "None\n",
      "Discarded 3944\n",
      "None\n",
      "Discarded 3945\n",
      "(163.31370849898477, 131.31370849898477)\n",
      "Done... 3946\n",
      "None\n",
      "Discarded 3947\n",
      "(232.0, 248.0)\n",
      "Done... 3948\n",
      "(264.0, 152.0)\n",
      "Done... 3949\n",
      "(254.62741699796953, 190.62741699796953)\n",
      "Done... 3950\n",
      "None\n",
      "Discarded 3951\n",
      "None\n",
      "Discarded 3952\n",
      "(264.0, 168.0)\n",
      "Done... 3953\n",
      "(277.856406460551, 229.85640646055103)\n",
      "Done... 3954\n",
      "None\n",
      "Discarded 3955\n",
      "None\n",
      "(217.88854381999832, 185.88854381999832)\n",
      "Done... 3956\n",
      "None\n",
      "Discarded 3957\n",
      "(248.0, 120.0)\n",
      "Done... 3958\n",
      "None\n",
      "Discarded 3959\n",
      "(123.59591794226543, 219.59591794226543)\n",
      "Discarded 3960\n",
      "(371.31370849898474, 323.31370849898474)\n",
      "Done... 3961\n",
      "(211.31370849898477, 147.31370849898477)\n",
      "Done... 3962\n",
      "(219.59591794226543, 155.59591794226543)\n",
      "Done... 3963\n",
      "(248.0, 216.0)\n",
      "Done... 3964\n",
      "None\n",
      "Discarded 3965\n",
      "(309.856406460551, 245.85640646055103)\n",
      "Done... 3966\n",
      "(296.0, 184.0)\n",
      "Done... 3967\n",
      "None\n",
      "None\n",
      "Discarded 3968\n",
      "(248.0, 232.0)\n",
      "Done... 3969\n",
      "(243.31370849898477, 291.31370849898474)\n",
      "Done... 3970\n",
      "(165.85640646055103, 117.85640646055101)\n",
      "Done... 3971\n",
      "None\n",
      "(200.0, 136.0)\n",
      "Done... 3972\n",
      "(232.0, 200.0)\n",
      "Done... 3973\n",
      "(163.31370849898477, 211.31370849898477)\n",
      "Done... 3974\n",
      "(233.88854381999832, 185.88854381999832)\n",
      "Done... 3975\n",
      "(179.31370849898477, 275.31370849898474)\n",
      "Done... 3976\n",
      "(184.0, 168.0)\n",
      "Done... 3977\n",
      "(115.31370849898477, 243.31370849898477)\n",
      "Discarded 3978\n",
      "(216.0, 184.0)\n",
      "Done... 3979\n",
      "None\n",
      "Discarded 3980\n",
      "(264.0, 200.0)\n",
      "Done... 3981\n",
      "(248.0, 232.0)\n",
      "Done... 3982\n",
      "(229.85640646055103, 213.85640646055103)\n",
      "Done... 3983\n",
      "(168.0, 248.0)\n",
      "Done... 3984\n",
      "(280.0, 296.0)\n",
      "Done... 3985\n",
      "(249.88854381999832, 233.88854381999832)\n",
      "Done... 3986\n",
      "(200.0, 200.0)\n",
      "Done... 3987\n",
      "None\n",
      "Discarded 3988\n",
      "(248.0, 216.0)\n",
      "Done... 3989\n",
      "None\n",
      "Discarded 3990\n",
      "(232.0, 184.0)\n",
      "Done... 3991\n",
      "None\n",
      "Discarded 3992\n",
      "(152.0, 248.0)\n",
      "Done... 3993\n",
      "(248.0, 168.0)\n",
      "Done... 3994\n",
      "(248.0, 200.0)\n",
      "Done... 3995\n",
      "None\n",
      "Discarded 3996\n",
      "(248.0, 248.0)\n",
      "Done... 3997\n",
      "(248.0, 184.0)\n",
      "Done... 3998\n",
      "(341.856406460551, 181.85640646055103)\n",
      "Done... 3999\n",
      "Number of images discarded from dataset:  2092\n",
      "***RAW IMAGES***\n",
      "[[1.23974041e+17 3.02721146e+17 3.38379156e+17 ... 9.95685552e+17\n",
      "  1.76135010e+17 2.04691606e+17]\n",
      " [2.99630520e+06 9.82481712e+06 1.09272758e+07 ... 2.19778875e+07\n",
      "  4.99896561e+06 4.84009166e+06]\n",
      " [2.08692956e+16 9.14826887e+16 1.03313369e+17 ... 2.13176565e+17\n",
      "  3.34462277e+16 3.66648960e+16]\n",
      " ...\n",
      " [4.05710777e+16 1.66152854e+17 1.86160947e+17 ... 4.00348206e+17\n",
      "  6.61989538e+16 6.19666352e+16]\n",
      " [3.33560131e+16 1.04680339e+17 1.16869161e+17 ... 2.35977553e+17\n",
      "  3.31143828e+16 4.03319953e+16]\n",
      " [3.26741699e+16 2.42193097e+17 2.12062428e+17 ... 6.13359269e+17\n",
      "  4.78899535e+16 9.46958019e+16]]\n"
     ]
    }
   ],
   "source": [
    "#print(y_train[8])\n",
    "\n",
    "print(len(fingerprints))\n",
    "\n",
    "rawImages = []\n",
    "classes = []\n",
    "failedCount = 0\n",
    "\n",
    "for i in range(len(fingerprints)):\n",
    "    featVec = handler(fingerprints[i], display=False)\n",
    "    if featVec is None:\n",
    "        failedCount+=1\n",
    "        print(\"Discarded\",i)\n",
    "        continue\n",
    "    \n",
    "    print(\"Done...\",i)\n",
    "    rawImages.append(featVec)\n",
    "    classes.append(labels[i])\n",
    "    \n",
    "rawImages = np.array(rawImages)\n",
    "classes = np.array(classes)\n",
    "\n",
    "print(\"Number of images discarded from dataset: \", failedCount)\n",
    "print(\"***RAW IMAGES***\")\n",
    "print(rawImages[:10])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(rawImages, classes, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('X_train.pickle','wb') as fe_data_file:\n",
    "     pickle.dump(X_train, fe_data_file)\n",
    "        \n",
    "with open('y_train.pickle','wb') as fe_data_file:\n",
    "     pickle.dump(y_train, fe_data_file)\n",
    "        \n",
    "with open('X_test.pickle','wb') as fe_data_file:\n",
    "     pickle.dump(X_test, fe_data_file)\n",
    "        \n",
    "with open('y_test.pickle','wb') as fe_data_file:\n",
    "     pickle.dump(y_test, fe_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('X_train.pickle','rb') as fe_data_file:\n",
    "     X_train = pickle.load(fe_data_file)\n",
    "\n",
    "with open('y_train.pickle','rb') as fe_data_file:\n",
    "     y_train = pickle.load(fe_data_file)\n",
    "        \n",
    "with open('X_test.pickle','rb') as fe_data_file:\n",
    "     X_test = pickle.load(fe_data_file)\n",
    "        \n",
    "with open('y_test.pickle','rb') as fe_data_file:\n",
    "     y_test = pickle.load(fe_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbours Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=150)\n",
    "neigh.fit(X_train,y_train)\n",
    "accuracy = neigh.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3] 0.39593908629441626\n"
     ]
    }
   ],
   "source": [
    "predicted = neigh.predict([X_test[0]])\n",
    "\n",
    "print(predicted,accuracy)\n",
    "\n",
    "prob_classes = neigh.predict_proba(X_test)\n",
    "two_classes = []\n",
    "\n",
    "idx=0\n",
    "for i in prob_classes:\n",
    "    l = np.argsort(i)\n",
    "    max_idx = l[-1]\n",
    "    second_max_idx = l[-2]\n",
    "    \n",
    "    two_classes.append([[X_test[idx],y_test[idx]],[max_idx, second_max_idx]])\n",
    "    idx+=1\n",
    "\n",
    "#print(two_classes)\n",
    "\n",
    "# from sklearn.neighbors import NearestNeighbors\n",
    "# neigh = NearestNeighbors(n_neighbors=2)\n",
    "# neigh.fit(X_train)\n",
    "# NearestNeighbors(n_neighbors=2)\n",
    "# print(neigh.kneighbors(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['W', 'R', 'L', 'T', 'A']\n"
     ]
    }
   ],
   "source": [
    "print(idx_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network based Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 different neural networks for 10 different pairwise classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "WA_Classifier = ['W','A']\n",
    "X_train_WA = []\n",
    "X_test_WA = []\n",
    "y_train_WA = []\n",
    "y_test_WA = []\n",
    "\n",
    "WR_Classifier = ['W','R']\n",
    "X_train_WR = []\n",
    "X_test_WR = []\n",
    "y_train_WR = []\n",
    "y_test_WR = []\n",
    "\n",
    "WT_Classifier = ['W','T']\n",
    "X_train_WT = []\n",
    "X_test_WT = []\n",
    "y_train_WT = []\n",
    "y_test_WT = []\n",
    "\n",
    "WL_Classifier = ['W','L']\n",
    "X_train_WL = []\n",
    "X_test_WL = []\n",
    "y_train_WL = []\n",
    "y_test_WL = []\n",
    "\n",
    "AR_Classifier = ['A','R']\n",
    "X_train_AR = []\n",
    "X_test_AR = []\n",
    "y_train_AR = []\n",
    "y_test_AR = []\n",
    "\n",
    "AT_Classifier = ['A','T']\n",
    "X_train_AT = []\n",
    "X_test_AT = []\n",
    "y_train_AT = []\n",
    "y_test_AT = []\n",
    "\n",
    "AL_Classifier = ['A','L']\n",
    "X_train_AL = []\n",
    "X_test_AL = []\n",
    "y_train_AL = []\n",
    "y_test_AL = []\n",
    "\n",
    "RT_Classifier = ['R','T']\n",
    "X_train_RT = []\n",
    "X_test_RT = []\n",
    "y_train_RT = []\n",
    "y_test_RT = []\n",
    "\n",
    "RL_Classifier = ['R','L']\n",
    "X_train_RL = []\n",
    "X_test_RL = []\n",
    "y_train_RL = []\n",
    "y_test_RL = []\n",
    "\n",
    "TL_Classifier = ['T','L']\n",
    "X_train_TL = []\n",
    "X_test_TL = []\n",
    "y_train_TL = []\n",
    "y_test_TL = []\n",
    "\n",
    "i=0\n",
    "\n",
    "# ['W', 'A', 'R', 'T', 'L']\n",
    "\n",
    "for x,y in zip(X_train,y_train):\n",
    "    if y==0 or y==1:\n",
    "        X_train_WA.append(x)\n",
    "        y_train_WA.append(y)\n",
    "    \n",
    "    if y==0 or y==2:\n",
    "        X_train_WR.append(x)\n",
    "        y_train_WR.append(y)\n",
    "    \n",
    "    if y==0 or y==3:\n",
    "        X_train_WT.append(x)\n",
    "        y_train_WT.append(y)\n",
    "        \n",
    "    if y==0 or y==4:\n",
    "        X_train_WL.append(x)\n",
    "        y_train_WL.append(y)\n",
    "        \n",
    "    if y==1 or y==2:\n",
    "        X_train_AR.append(x)\n",
    "        y_train_AR.append(y)\n",
    "        \n",
    "    if y==1 or y==3:\n",
    "        X_train_AT.append(x)\n",
    "        y_train_AT.append(y)\n",
    "        \n",
    "    if y==1 or y==4:\n",
    "        X_train_AL.append(x)\n",
    "        y_train_AL.append(y)\n",
    "        \n",
    "    if y==2 or y==3:\n",
    "        X_train_RT.append(x)\n",
    "        y_train_RT.append(y)\n",
    "        \n",
    "    if y==2 or y==4:\n",
    "        X_train_RL.append(x)\n",
    "        y_train_RL.append(y)\n",
    "        \n",
    "    if y==3 or y==4:\n",
    "        X_train_TL.append(x)\n",
    "        y_train_TL.append(y)        \n",
    "\n",
    "\n",
    "#['W', 'A', 'R', 'T', 'L']\n",
    "\n",
    "for test,classes in two_classes:\n",
    "    if set(classes) == set([0,1]):\n",
    "        X_test_WA.append(test[0])\n",
    "        y_test_WA.append(test[1])\n",
    "        \n",
    "    if set(classes) == set([0,2]):\n",
    "        X_test_WR.append(test[0])\n",
    "        y_test_WR.append(test[1])\n",
    "    \n",
    "    if set(classes) == set([0,3]):\n",
    "        X_test_WT.append(test[0])\n",
    "        y_test_WT.append(test[1])\n",
    "        \n",
    "    if set(classes) == set([0,4]):\n",
    "        X_test_WL.append(test[0])\n",
    "        y_test_WL.append(test[1])\n",
    "        \n",
    "    if set(classes) == set([1,2]):\n",
    "        X_test_AR.append(test[0])\n",
    "        y_test_AR.append(test[1])\n",
    "        \n",
    "    if set(classes) == set([1,3]):\n",
    "        X_test_AT.append(test[0])\n",
    "        y_test_AT.append(test[1])\n",
    "        \n",
    "    if set(classes) == set([1,4]):\n",
    "        X_test_AL.append(test[0])\n",
    "        y_test_AL.append(test[1])\n",
    "        \n",
    "    if set(classes) == set([2,3]):\n",
    "        X_test_RT.append(test[0])\n",
    "        y_test_RT.append(test[1])\n",
    "    \n",
    "    if set(classes) == set([2,4]):\n",
    "        X_test_RL.append(test[0])\n",
    "        y_test_RL.append(test[1])\n",
    "        \n",
    "    if set(classes) == set([3,4]):\n",
    "        X_test_TL.append(test[0])\n",
    "        y_test_TL.append(test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, activation = 'relu', input_shape = X_train.shape[1:]))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "    model.compile(optimizer = 'adam', loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_WA = true_WR = true_WT = true_WL = true_AR = true_AT = true_AL = true_RT = true_RL = true_TL = []\n",
    "prediction_WA = prediction_WR = prediction_WT = prediction_WL = prediction_AR = prediction_AT = prediction_AL = prediction_RT = prediction_RL = prediction_TL = []\n",
    "\n",
    "# WA_Classifier\n",
    "X_train_WA = np.array(X_train_WA)\n",
    "y_train_WA = np.array(y_train_WA)\n",
    "X_test_WA = np.array(X_test_WA)\n",
    "model = train_model(X_train_WA)\n",
    "print(X_train_WA.shape,y_train_WA.shape)\n",
    "X_train_WA.shape[1:]\n",
    "model.fit(X_train_WA, y_train_WA, epochs=10)\n",
    "\n",
    "if len(X_test_WA)>0:\n",
    "    y_pred = model.predict(X_test_WA)\n",
    "    y_classes = [np.argmax(element) for element in y_pred]\n",
    "    prediction_WA = [WA_Classifier[i] for i in y_classes]\n",
    "    true_WA = [idx_labels[i] for i in y_test_WA]\n",
    "\n",
    "\n",
    "# WR_Classifier\n",
    "X_train_WR = np.array(X_train_WR)\n",
    "y_train_WR = np.array(y_train_WR)\n",
    "X_test_WR = np.array(X_test_WR)\n",
    "model = train_model(X_train_WR)\n",
    "model.fit(X_train_WR, y_train_WR, epochs=10)\n",
    "\n",
    "if len(X_test_WR)>0:\n",
    "    y_pred = model.predict(X_test_WR)\n",
    "    y_classes = [np.argmax(element) for element in y_pred]\n",
    "    prediction_WR = [WR_Classifier[i] for i in y_classes]\n",
    "    true_WR = [idx_labels[i] for i in y_test_WR]\n",
    "\n",
    "\n",
    "# WT_Classifier\n",
    "X_train_WT = np.array(X_train_WT)\n",
    "y_train_WT = np.array(y_train_WT)\n",
    "X_test_WT = np.array(X_test_WT)\n",
    "model = train_model(X_train_WT)\n",
    "model.fit(X_train_WT, y_train_WT, epochs=10)\n",
    "\n",
    "if len(X_test_WT)>0:\n",
    "    y_pred = model.predict(X_test_WT)\n",
    "    y_classes = [np.argmax(element) for element in y_pred]\n",
    "    prediction_WT = [WT_Classifier[i] for i in y_classes]\n",
    "    true_WT = [idx_labels[i] for i in y_test_WT]\n",
    "\n",
    "\n",
    "# WL_Classifier\n",
    "X_train_WL = np.array(X_train_WL)\n",
    "y_train_WL = np.array(y_train_WL)\n",
    "X_test_WL = np.array(X_test_WL)\n",
    "model = train_model(X_train_WL)\n",
    "model.fit(X_train_WL, y_train_WL, epochs=10)\n",
    "\n",
    "if len(X_test_WL)>0:\n",
    "    y_pred = model.predict(X_test_WL)\n",
    "    y_classes = [np.argmax(element) for element in y_pred]\n",
    "    prediction_WL = [WL_Classifier[i] for i in y_classes]\n",
    "    true_WL = [idx_labels[i] for i in y_test_WL]\n",
    "\n",
    "\n",
    "# AR_Classifier\n",
    "X_train_AR = np.array(X_train_AR)\n",
    "y_train_AR = np.array(y_train_AR)\n",
    "X_test_AR = np.array(X_test_AR)\n",
    "model = train_model(X_train_AR)\n",
    "model.fit(X_train_AR, y_train_AR, epochs=10)\n",
    "\n",
    "\n",
    "if len(X_test_AR)>0:\n",
    "    y_pred = model.predict(X_test_AR)\n",
    "    y_classes = [np.argmax(element) for element in y_pred]\n",
    "    prediction_AR = [AR_Classifier[i] for i in y_classes]\n",
    "    true_AR = [idx_labels[i] for i in y_test_AR]\n",
    "\n",
    "\n",
    "# AT_Classifier\n",
    "X_train_AT = np.array(X_train_AT)\n",
    "y_train_AT = np.array(y_train_AT)\n",
    "X_test_AT = np.array(X_test_AT)\n",
    "model = train_model(X_train_AT)\n",
    "model.fit(X_train_AT, y_train_AT, epochs=10)\n",
    "\n",
    "if len(X_test_AT)>0:\n",
    "    y_pred = model.predict(X_test_AT)\n",
    "    y_classes = [np.argmax(element) for element in y_pred]\n",
    "    prediction_AT = [AT_Classifier[i] for i in y_classes]\n",
    "    true_AT = [idx_labels[i] for i in y_test_AT]\n",
    "\n",
    "\n",
    "# AL_Classifier\n",
    "X_train_AL = np.array(X_train_AL)\n",
    "y_train_AL = np.array(y_train_AL)\n",
    "X_test_AL = np.array(X_test_AL)\n",
    "model = train_model(X_train_AL)\n",
    "model.fit(X_train_AL, y_train_AL, epochs=10)\n",
    "\n",
    "if len(X_test_AL)>0:\n",
    "    y_pred = model.predict(X_test_AL)\n",
    "    y_classes = [np.argmax(element) for element in y_pred]\n",
    "    prediction_AL = [AL_Classifier[i] for i in y_classes]\n",
    "    true_AL = [idx_labels[i] for i in y_test_AL]\n",
    "\n",
    "\n",
    "# RT_Classifier\n",
    "X_train_RT = np.array(X_train_RT)\n",
    "y_train_RT = np.array(y_train_RT)\n",
    "X_test_RT = np.array(X_test_RT)\n",
    "model = train_model(X_train_RT)\n",
    "model.fit(X_train_RT, y_train_RT, epochs=10)\n",
    "\n",
    "if len(X_test_RT)>0:\n",
    "    y_pred = model.predict(X_test_RT)\n",
    "    y_classes = [np.argmax(element) for element in y_pred]\n",
    "    prediction_RT = [RT_Classifier[i] for i in y_classes]\n",
    "    true_RT = [idx_labels[i] for i in y_test_RT]\n",
    "\n",
    "\n",
    "# RL_Classifier\n",
    "X_train_RL = np.array(X_train_RL)\n",
    "y_train_RL = np.array(y_train_RL)\n",
    "X_test_RL = np.array(X_test_RL)\n",
    "model = train_model(X_train_RL)\n",
    "model.fit(X_train_RL, y_train_RL, epochs=10)\n",
    "\n",
    "if len(X_test_RL)>0:\n",
    "    y_pred = model.predict(X_test_RL)\n",
    "    y_classes = [np.argmax(element) for element in y_pred]\n",
    "    prediction_RL = [RL_Classifier[i] for i in y_classes]\n",
    "    true_RL = [idx_labels[i] for i in y_test_RL]\n",
    "\n",
    "\n",
    "# TL_Classifier\n",
    "X_train_TL = np.array(X_train_TL)\n",
    "y_train_TL = np.array(y_train_TL)\n",
    "X_test_TL = np.array(X_test_TL)\n",
    "model = train_model(X_train_TL)\n",
    "model.fit(X_train_TL, y_train_TL, epochs=10)\n",
    "\n",
    "if len(X_test_TL)>0:\n",
    "    y_pred = model.predict(X_test_TL)\n",
    "    y_classes = [np.argmax(element) for element in y_pred]\n",
    "    prediction_TL = [TL_Classifier[i] for i in y_classes]\n",
    "    true_TL = [idx_labels[i] for i in y_test_TL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Accuracy =  0.23688663282571912 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import pylab as pl\n",
    "\n",
    "prediction_classes = prediction_WA + prediction_WR + prediction_WT + prediction_WL + prediction_AR + prediction_AT + prediction_AL + prediction_RT + prediction_RL + prediction_TL\n",
    "\n",
    "true_classes = true_WA + true_WR + true_WT + true_WL + true_AR + true_AT + true_AL + true_RT + true_RL + true_TL\n",
    "\n",
    "# print(len(true_classes), len(prediction_classes))\n",
    "# print(\"True Class\",\",\", \"Predicted Class\")\n",
    "# print(\"_______________________________\\n\")\n",
    "# for i in range(len(true_classes)):\n",
    "#     print((true_classes[i],prediction_classes[i]))\n",
    "        \n",
    "accuracy = accuracy_score(true_classes, prediction_classes)\n",
    "print (\"\\nFinal Accuracy = \", accuracy,\"\\n\")\n",
    "cm = confusion_matrix(true_classes, prediction_classes)\n",
    "pl.matshow(cm)\n",
    "pl.colorbar()\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
